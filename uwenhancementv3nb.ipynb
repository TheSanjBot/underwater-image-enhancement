{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 10951228,
          "sourceType": "datasetVersion",
          "datasetId": 6812104
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "uwenhancementv3nb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheSanjBot/underwater-image-enhancement/blob/main/uwenhancementv3nb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "SLcFoSgALIA_"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "sanjayseven_uwenhancementv3_path = kagglehub.dataset_download('sanjayseven/uwenhancementv3')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "KiQ9sKUlLIBA"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS517 Project - UnderWater Image Enhancement\n",
        "## Pranjali Bajpai - 2018EEB1243\n",
        "## Yogesh Vaidhya - 2018EEB1277\n"
      ],
      "metadata": {
        "id": "IwNjnpG4LIBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from PIL import Image\n",
        "\n",
        "# Load image\n",
        "image1 = Image.open('/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/dataset/raw/Img1.png')\n",
        "image2 = Image.open('/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/dataset/raw/Img2.png')\n",
        "image3 = Image.open('/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/dataset/raw/Img3.png')\n",
        "image4 = Image.open('/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/dataset/raw/Img4.png')\n",
        "image5 = Image.open('/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/dataset/raw/Img5.png')\n",
        "image6 = Image.open('/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/dataset/raw/Img6.png')\n",
        "image7 = Image.open('/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/dataset/raw/Img7.png')\n",
        "image8 = Image.open('/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/dataset/raw/Img8.jpg')\n",
        "image9 = Image.open('/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/dataset/raw/Img9.jpg')\n",
        "image10 = Image.open('/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/dataset/raw/Img10.jpg')\n",
        "# Load reference images\n",
        "rimage1 = Image.open('/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/dataset/reference/RImg1.png')\n",
        "rimage2 = Image.open('/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/dataset/reference/RImg2.png')\n",
        "rimage3 = Image.open('/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/dataset/reference/RImg3.png')\n",
        "rimage4 = Image.open('/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/dataset/reference/RImg4.png')\n",
        "rimage5 = Image.open('/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/dataset/reference/RImg5.png')\n",
        "rimage6 = Image.open('/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/dataset/reference/RImg6.png')\n",
        "rimage7 = Image.open('/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/dataset/reference/RImg7.png')\n",
        "rimage8 = Image.open('/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/dataset/reference/Img8.jpg')\n",
        "rimage9 = Image.open('/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/dataset/reference/Img9.jpg')\n",
        "rimage10 = Image.open('/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/dataset/reference/Img10.jpg')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T17:43:52.929321Z",
          "iopub.execute_input": "2025-03-07T17:43:52.929618Z",
          "iopub.status.idle": "2025-03-07T17:43:53.332508Z",
          "shell.execute_reply.started": "2025-03-07T17:43:52.929595Z",
          "shell.execute_reply": "2025-03-07T17:43:53.331856Z"
        },
        "id": "Vfj4lZWQLIBC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cupy-cuda11x  # Install CuPy for Kaggle's CUDA 11.x"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T10:38:45.196772Z",
          "iopub.execute_input": "2025-03-07T10:38:45.197046Z",
          "iopub.status.idle": "2025-03-07T10:38:53.50831Z",
          "shell.execute_reply.started": "2025-03-07T10:38:45.197025Z",
          "shell.execute_reply": "2025-03-07T10:38:53.507238Z"
        },
        "id": "i7xplQLKLIBC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T11:44:43.712945Z",
          "iopub.execute_input": "2025-03-07T11:44:43.713226Z",
          "iopub.status.idle": "2025-03-07T11:44:46.74704Z",
          "shell.execute_reply.started": "2025-03-07T11:44:43.713205Z",
          "shell.execute_reply": "2025-03-07T11:44:46.746362Z"
        },
        "id": "YKW9k2K4LIBD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T11:44:48.807067Z",
          "iopub.execute_input": "2025-03-07T11:44:48.807499Z",
          "iopub.status.idle": "2025-03-07T11:44:48.891739Z",
          "shell.execute_reply.started": "2025-03-07T11:44:48.807473Z",
          "shell.execute_reply": "2025-03-07T11:44:48.890863Z"
        },
        "id": "d1-y2RQ3LIBD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def rgb_to_hsv(image_tensor):\n",
        "    \"\"\"Convert RGB tensor to HSV space (PyTorch implementation)\"\"\"\n",
        "    img = image_tensor / 255.0\n",
        "    r, g, b = img[0], img[1], img[2]\n",
        "\n",
        "    maxc, _ = torch.max(img, dim=0)\n",
        "    minc, _ = torch.min(img, dim=0)\n",
        "    delta = maxc - minc\n",
        "\n",
        "    h = torch.zeros_like(maxc)\n",
        "    s = torch.zeros_like(maxc)\n",
        "    v = maxc\n",
        "\n",
        "    mask = delta != 0\n",
        "    rc = (maxc - r) / (delta + 1e-6)\n",
        "    gc = (maxc - g) / (delta + 1e-6)\n",
        "    bc = (maxc - b) / (delta + 1e-6)\n",
        "\n",
        "    h = torch.where((maxc == r) & mask, bc - gc, h)\n",
        "    h = torch.where((maxc == g) & mask, 2.0 + rc - bc, h)\n",
        "    h = torch.where((maxc == b) & mask, 4.0 + gc - rc, h)\n",
        "    h = (h / 6.0) % 1.0\n",
        "\n",
        "    s = torch.where(maxc > 0, delta / maxc, 0.0)\n",
        "\n",
        "    h = h * 179.0\n",
        "    s = s * 255.0\n",
        "    v = v * 255.0\n",
        "\n",
        "    return torch.stack([h, s, v], dim=0)\n",
        "\n",
        "def hsv_to_rgb(hsv_tensor):\n",
        "    \"\"\"Convert HSV tensor to RGB space (PyTorch implementation)\"\"\"\n",
        "    h, s, v = hsv_tensor[0]/179.0, hsv_tensor[1]/255.0, hsv_tensor[2]/255.0\n",
        "\n",
        "    c = v * s\n",
        "    x = c * (1 - torch.abs((h * 6) % 2 - 1))\n",
        "    m = v - c\n",
        "\n",
        "    h_prime = (h * 6) % 6\n",
        "    rgb = torch.zeros_like(hsv_tensor)\n",
        "\n",
        "    cond = (h_prime >= 0) & (h_prime < 1)\n",
        "    rgb[0] = torch.where(cond, c, rgb[0])\n",
        "    rgb[1] = torch.where(cond, x, rgb[1])\n",
        "\n",
        "    cond = (h_prime >= 1) & (h_prime < 2)\n",
        "    rgb[0] = torch.where(cond, x, rgb[0])\n",
        "    rgb[1] = torch.where(cond, c, rgb[1])\n",
        "\n",
        "    cond = (h_prime >= 2) & (h_prime < 3)\n",
        "    rgb[1] = torch.where(cond, c, rgb[1])\n",
        "    rgb[2] = torch.where(cond, x, rgb[2])\n",
        "\n",
        "    cond = (h_prime >= 3) & (h_prime < 4)\n",
        "    rgb[1] = torch.where(cond, x, rgb[1])\n",
        "    rgb[2] = torch.where(cond, c, rgb[2])\n",
        "\n",
        "    cond = (h_prime >= 4) & (h_prime < 5)\n",
        "    rgb[0] = torch.where(cond, x, rgb[0])\n",
        "    rgb[2] = torch.where(cond, c, rgb[2])\n",
        "\n",
        "    cond = (h_prime >= 5) & (h_prime < 6)\n",
        "    rgb[0] = torch.where(cond, c, rgb[0])\n",
        "    rgb[2] = torch.where(cond, x, rgb[2])\n",
        "\n",
        "    rgb = (rgb + m) * 255.0\n",
        "    return torch.clamp(rgb, 0.0, 255.0)\n",
        "\n",
        "def compensate_RB_gpu(image_tensor, flag):\n",
        "    \"\"\"GPU-accelerated channel compensation\"\"\"\n",
        "    R, G, B = image_tensor[0], image_tensor[1], image_tensor[2]\n",
        "\n",
        "    minR, maxR = torch.min(R), torch.max(R)\n",
        "    minG, maxG = torch.min(G), torch.max(G)\n",
        "    minB, maxB = torch.min(B), torch.max(B)\n",
        "\n",
        "    R_norm = (R - minR) / (maxR - minR + 1e-6)\n",
        "    G_norm = (G - minG) / (maxG - minG + 1e-6)\n",
        "    B_norm = (B - minB) / (maxB - minB + 1e-6)\n",
        "\n",
        "    meanR, meanG, meanB = torch.mean(R_norm), torch.mean(G_norm), torch.mean(B_norm)\n",
        "\n",
        "    if flag == 0:\n",
        "        R_comp = (R_norm + (meanG - meanR) * (1 - R_norm) * G_norm) * maxR\n",
        "        B_comp = (B_norm + (meanG - meanB) * (1 - B_norm) * G_norm) * maxB\n",
        "        G_comp = G_norm * maxG\n",
        "    else:\n",
        "        R_comp = (R_norm + (meanG - meanR) * (1 - R_norm) * G_norm) * maxR\n",
        "        B_comp = B_norm * maxB\n",
        "        G_comp = G_norm * maxG\n",
        "\n",
        "    return torch.stack([\n",
        "        torch.clamp(R_comp, 0, 255),\n",
        "        torch.clamp(G_comp, 0, 255),\n",
        "        torch.clamp(B_comp, 0, 255)\n",
        "    ], dim=0)\n",
        "\n",
        "def gray_world_gpu(image_tensor):\n",
        "    \"\"\"GPU-accelerated gray world white balancing\"\"\"\n",
        "    R, G, B = image_tensor[0], image_tensor[1], image_tensor[2]\n",
        "    gray = 0.299 * R + 0.587 * G + 0.114 * B\n",
        "    mean_gray = torch.mean(gray)\n",
        "\n",
        "    R_new = R * (mean_gray / torch.mean(R))\n",
        "    G_new = G * (mean_gray / torch.mean(G))\n",
        "    B_new = B * (mean_gray / torch.mean(B))\n",
        "\n",
        "    return torch.stack([\n",
        "        torch.clamp(R_new, 0, 255),\n",
        "        torch.clamp(G_new, 0, 255),\n",
        "        torch.clamp(B_new, 0, 255)\n",
        "    ], dim=0)\n",
        "\n",
        "def equalize_gpu(v_channel):\n",
        "    \"\"\"GPU histogram equalization for V channel\"\"\"\n",
        "    hist = torch.histc(v_channel, bins=256, min=0, max=255)\n",
        "    cdf = torch.cumsum(hist, 0)\n",
        "    cdf_min = cdf[cdf > 0].min()\n",
        "    cdf = (cdf - cdf_min) / (v_channel.numel() - cdf_min + 1e-6)\n",
        "    lut = (cdf * 255).clamp(0, 255)\n",
        "    return lut[v_channel.to(torch.long).clamp(0, 255)]\n",
        "\n",
        "def hsv_global_equalization_gpu(image_tensor):\n",
        "    \"\"\"GPU-accelerated HSV equalization\"\"\"\n",
        "    hsv = rgb_to_hsv(image_tensor)\n",
        "    v_eq = equalize_gpu(hsv[2].to(torch.float32))\n",
        "    hsv_eq = torch.stack([hsv[0], hsv[1], v_eq], dim=0)\n",
        "    return hsv_to_rgb(hsv_eq)\n",
        "\n",
        "def sharpen_gpu(image_tensor):\n",
        "    \"\"\"GPU-accelerated sharpening\"\"\"\n",
        "    kernel = torch.tensor([[1, 4, 6, 4, 1],\n",
        "                          [4, 16, 24, 16, 4],\n",
        "                          [6, 24, 36, 24, 6],\n",
        "                          [4, 16, 24, 16, 4],\n",
        "                          [1, 4, 6, 4, 1]], dtype=torch.float32, device=device) / 256.0\n",
        "    kernel = kernel.repeat(3, 1, 1, 1)\n",
        "    blurred = torch.nn.functional.conv2d(image_tensor.unsqueeze(0), kernel, padding=2, groups=3)\n",
        "    return torch.clamp(1.5 * image_tensor - 0.5 * blurred.squeeze(0), 0, 255)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T17:41:45.313517Z",
          "iopub.execute_input": "2025-03-07T17:41:45.313862Z",
          "iopub.status.idle": "2025-03-07T17:41:50.889322Z",
          "shell.execute_reply.started": "2025-03-07T17:41:45.313826Z",
          "shell.execute_reply": "2025-03-07T17:41:50.88836Z"
        },
        "id": "Fm5BWVwDLIBE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def underwater_image_enhancement(image, flag=1, scale_factor=1.0):\n",
        "    \"\"\"\n",
        "    Enhanced GPU-accelerated version of the original function\n",
        "    Supports both PIL Images and numpy arrays\n",
        "    \"\"\"\n",
        "    # Convert input to tensor\n",
        "    if isinstance(image, Image.Image):\n",
        "        pil_input = True\n",
        "        image_np = np.array(image)\n",
        "        if scale_factor != 1.0:\n",
        "            new_size = (int(image.size[0]*scale_factor)), (int(image.size[1]*scale_factor))\n",
        "            image_np = np.array(image.resize(new_size, Image.Resampling.LANCZOS))\n",
        "    else:\n",
        "        pil_input = False\n",
        "        image_np = image.copy()\n",
        "        if scale_factor != 1.0:\n",
        "            h, w = image_np.shape[:2]\n",
        "            new_size = (int(w*scale_factor), int(h*scale_factor))\n",
        "            image_np = cv2.resize(image_np, new_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # Convert to tensor and process\n",
        "    tensor = torch.from_numpy(image_np).permute(2, 0, 1).float().to(device)\n",
        "\n",
        "    # Processing pipeline\n",
        "    compensated = compensate_RB_gpu(tensor, flag)\n",
        "    whitebalanced = gray_world_gpu(compensated)\n",
        "    contrastenhanced = hsv_global_equalization_gpu(whitebalanced)\n",
        "    sharpened = sharpen_gpu(whitebalanced)\n",
        "\n",
        "    # Fusion\n",
        "    fused = torch.clamp(0.5*contrastenhanced + 0.5*sharpened, 0, 255)\n",
        "\n",
        "    # Convert back to original format\n",
        "    output_np = fused.permute(1, 2, 0).byte().cpu().numpy()\n",
        "\n",
        "    if pil_input:\n",
        "        return Image.fromarray(output_np)\n",
        "    return output_np"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T17:42:23.480026Z",
          "iopub.execute_input": "2025-03-07T17:42:23.480319Z",
          "iopub.status.idle": "2025-03-07T17:42:23.486719Z",
          "shell.execute_reply.started": "2025-03-07T17:42:23.480296Z",
          "shell.execute_reply": "2025-03-07T17:42:23.486075Z"
        },
        "id": "_wTtUN2fLIBF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "averagefused1 = underwater_image_enhancement(image1, scale_factor=0.75)\n",
        "\n",
        "averagefused1.save('/kaggle/working/averagefused1.png')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T17:44:01.931828Z",
          "iopub.execute_input": "2025-03-07T17:44:01.932144Z",
          "iopub.status.idle": "2025-03-07T17:44:03.212357Z",
          "shell.execute_reply.started": "2025-03-07T17:44:01.93212Z",
          "shell.execute_reply": "2025-03-07T17:44:03.211662Z"
        },
        "id": "woQIdOhGLIBF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For maximum speed (lowest quality)\n",
        "enhance_video(input_path, output_path, scale_factor=0.4)\n",
        "\n",
        "# For balanced quality/speed\n",
        "enhance_video(input_path, output_path, scale_factor=0.67)\n",
        "\n",
        "# For near-original quality\n",
        "enhance_video(input_path, output_path, scale_factor=0.85)"
      ],
      "metadata": {
        "id": "RcmRFjZlLIBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_frame_gpu(frame, flag=1):\n",
        "    \"\"\"GPU-accelerated frame processing pipeline\"\"\"\n",
        "    tensor = torch.from_numpy(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) \\\n",
        "              .permute(2, 0, 1).float().to(device)\n",
        "\n",
        "    compensated = compensate_RB_gpu(tensor, flag)\n",
        "    whitebalanced = gray_world_gpu(compensated)\n",
        "    contrastenhanced = hsv_global_equalization_gpu(whitebalanced)\n",
        "    sharpened = sharpen_gpu(whitebalanced)\n",
        "\n",
        "    fused = torch.clamp(0.5 * contrastenhanced + 0.5 * sharpened, 0, 255)\n",
        "    return cv2.cvtColor(fused.permute(1, 2, 0).byte().cpu().numpy(), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "def enhance_video(input_path, output_path, flag=1, scale_factor=1.0):\n",
        "    \"\"\"Main video enhancement function with progress tracking\"\"\"\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Cannot open video file.\")\n",
        "        return\n",
        "\n",
        "    # Get video properties\n",
        "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Calculate scaled dimensions\n",
        "    new_width = int(original_width * scale_factor)\n",
        "    new_height = int(original_height * scale_factor)\n",
        "    new_width -= new_width % 2\n",
        "    new_height -= new_height % 2\n",
        "\n",
        "    # Initialize video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (new_width, new_height))\n",
        "\n",
        "    current_frame = 0\n",
        "    print(f\"Total frames to process: {total_frames}\")\n",
        "    print(\"Progress: [\", end='', flush=True)\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "\n",
        "        # Process frame\n",
        "        frame = cv2.resize(frame, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
        "        processed_frame = process_frame_gpu(frame, flag)\n",
        "        out.write(processed_frame)\n",
        "\n",
        "        # Update progress\n",
        "        current_frame += 1\n",
        "        progress = current_frame / total_frames\n",
        "        if current_frame % max(1, total_frames//100) == 0:  # Update every 1% or 1 frame\n",
        "            print(\"#\", end='', flush=True)\n",
        "\n",
        "    # Cleanup\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(\"]\\nProcessing completed!\")\n",
        "    print(f\"Enhanced video saved to: {output_path}\")\n",
        "\n",
        "# Example usage with progress tracking\n",
        "enhance_video(\n",
        "    \"/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/GX016431.MP4\",\n",
        "    \"/kaggle/working/output.mp4\",\n",
        "    flag=1,\n",
        "    scale_factor=0.5\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T17:46:37.46001Z",
          "iopub.execute_input": "2025-03-07T17:46:37.46037Z",
          "iopub.status.idle": "2025-03-07T17:48:23.081698Z",
          "shell.execute_reply.started": "2025-03-07T17:46:37.460342Z",
          "shell.execute_reply": "2025-03-07T17:48:23.080874Z"
        },
        "id": "QMdX5dPELIBF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**code v2**"
      ],
      "metadata": {
        "id": "YjpVXi0pLIBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "def compensate_RB(image_np, flag):\n",
        "    \"\"\"Compensates Red and Blue channels in an image to improve underwater visibility.\"\"\"\n",
        "    image = Image.fromarray(image_np)  # Convert to PIL\n",
        "\n",
        "    imager, imageg, imageb = image.split()\n",
        "    minR, maxR = imager.getextrema()\n",
        "    minG, maxG = imageg.getextrema()\n",
        "    minB, maxB = imageb.getextrema()\n",
        "\n",
        "    # Convert to NumPy arrays\n",
        "    imageR = np.array(imager, np.float64)\n",
        "    imageG = np.array(imageg, np.float64)\n",
        "    imageB = np.array(imageb, np.float64)\n",
        "\n",
        "    # Normalize (Avoid divide by zero)\n",
        "    imageR = (imageR - minR) / (maxR - minR + 1e-6)\n",
        "    imageG = (imageG - minG) / (maxG - minG + 1e-6)\n",
        "    imageB = (imageB - minB) / (maxB - minB + 1e-6)\n",
        "\n",
        "    # Get mean values\n",
        "    meanR, meanG, meanB = np.mean(imageR), np.mean(imageG), np.mean(imageB)\n",
        "\n",
        "    # Apply Compensation\n",
        "    if flag == 0:\n",
        "        imageR = (imageR + (meanG - meanR) * (1 - imageR) * imageG) * maxR\n",
        "        imageB = (imageB + (meanG - meanB) * (1 - imageB) * imageG) * maxB\n",
        "        imageG *= maxG\n",
        "\n",
        "    elif flag == 1:\n",
        "        imageR = (imageR + (meanG - meanR) * (1 - imageR) * imageG) * maxR\n",
        "        imageB *= maxB\n",
        "        imageG *= maxG\n",
        "\n",
        "    # Convert back to uint8\n",
        "    return np.stack([\n",
        "        np.clip(imageR, 0, 255).astype(np.uint8),\n",
        "        np.clip(imageG, 0, 255).astype(np.uint8),\n",
        "        np.clip(imageB, 0, 255).astype(np.uint8)\n",
        "    ], axis=-1)\n",
        "\n",
        "def gray_world(image_np):\n",
        "    \"\"\"Applies the Gray World Algorithm for white balance.\"\"\"\n",
        "    image = Image.fromarray(image_np)\n",
        "\n",
        "    imager, imageg, imageb = image.split()\n",
        "    imageGray = image.convert('L')\n",
        "\n",
        "    # Convert to NumPy\n",
        "    imageR = np.array(imager, np.float64)\n",
        "    imageG = np.array(imageg, np.float64)\n",
        "    imageB = np.array(imageb, np.float64)\n",
        "    meanGray = np.mean(np.array(imageGray, np.float64))\n",
        "\n",
        "    # Mean Values\n",
        "    meanR, meanG, meanB = np.mean(imageR), np.mean(imageG), np.mean(imageB)\n",
        "\n",
        "    # Apply Gray World\n",
        "    return np.stack([\n",
        "        np.clip(imageR * meanGray / meanR, 0, 255).astype(np.uint8),\n",
        "        np.clip(imageG * meanGray / meanG, 0, 255).astype(np.uint8),\n",
        "        np.clip(imageB * meanGray / meanB, 0, 255).astype(np.uint8)\n",
        "    ], axis=-1)\n",
        "\n",
        "def hsv_global_equalization(image_np):\n",
        "    \"\"\"Applies Histogram Equalization in HSV space to enhance contrast.\"\"\"\n",
        "    hsvimage = cv2.cvtColor(image_np, cv2.COLOR_RGB2HSV)\n",
        "    hsvimage[:, :, 2] = cv2.equalizeHist(hsvimage[:, :, 2])\n",
        "    return cv2.cvtColor(hsvimage, cv2.COLOR_HSV2RGB)\n",
        "\n",
        "def sharpen(image_np):\n",
        "    \"\"\"Sharpens the image using Gaussian Blur.\"\"\"\n",
        "    blurred = cv2.GaussianBlur(image_np, (5, 5), 1.0)\n",
        "    return cv2.addWeighted(image_np, 1.5, blurred, -0.5, 0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def underwater_image_enhancement(image, refimage, flag):\n",
        "    # Convert PIL Image to numpy array\n",
        "    if isinstance(image, Image.Image):\n",
        "        pil_input = True\n",
        "        image_np = np.array(image)\n",
        "    else:\n",
        "        pil_input = False\n",
        "        image_np = image\n",
        "\n",
        "    # Processing pipeline\n",
        "    compensated = compensate_RB(image_np, flag)\n",
        "    whitebalanced = gray_world(compensated)\n",
        "    contrastenhanced = hsv_global_equalization(whitebalanced)\n",
        "    sharpened = sharpen(whitebalanced)\n",
        "\n",
        "    # Fusion and output conversion\n",
        "    averagefused = cv2.addWeighted(contrastenhanced, 0.5, sharpened, 0.5, 0)\n",
        "\n",
        "    # Convert back to PIL if input was PIL\n",
        "    return Image.fromarray(averagefused) if pil_input else averagefused"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T17:57:23.834445Z",
          "iopub.execute_input": "2025-03-07T17:57:23.834769Z",
          "iopub.status.idle": "2025-03-07T17:57:23.847147Z",
          "shell.execute_reply.started": "2025-03-07T17:57:23.834745Z",
          "shell.execute_reply": "2025-03-07T17:57:23.846282Z"
        },
        "id": "1_7MspYWLIBG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "averagefused1 = underwater_image_enhancement(image1, None, 1)\n",
        "\n",
        "averagefused1.save('/kaggle/working/averagefused1.png')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T17:59:17.158211Z",
          "iopub.execute_input": "2025-03-07T17:59:17.158524Z",
          "iopub.status.idle": "2025-03-07T17:59:17.509143Z",
          "shell.execute_reply.started": "2025-03-07T17:59:17.158501Z",
          "shell.execute_reply": "2025-03-07T17:59:17.508426Z"
        },
        "id": "BbC8VkH6LIBG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def process_frame(frame, flag=1):\n",
        "    \"\"\"Processes a single frame through the underwater enhancement pipeline.\"\"\"\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "    compensated = compensate_RB(frame, flag)\n",
        "    whitebalanced = gray_world(compensated)\n",
        "    contrastenhanced = hsv_global_equalization(whitebalanced)\n",
        "    sharpened = sharpen(whitebalanced)\n",
        "\n",
        "    # Fusion of contrast-enhanced and sharpened frames\n",
        "    output_frame = cv2.addWeighted(contrastenhanced, 0.5, sharpened, 0.5, 0)\n",
        "\n",
        "    return cv2.cvtColor(output_frame, cv2.COLOR_RGB2BGR)  # Convert back to BGR\n",
        "\n",
        "def enhance_video(input_path, output_path, flag=1):\n",
        "    \"\"\"Enhances an underwater video with progress tracking\"\"\"\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Cannot open video file.\")\n",
        "        return\n",
        "\n",
        "    # Get video properties\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    current_frame = 0\n",
        "    print(f\"Total frames to process: {total_frames}\")\n",
        "    print(\"Progress: [\", end='', flush=True)\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break  # Stop if no more frames\n",
        "\n",
        "        enhanced_frame = process_frame(frame, flag)\n",
        "        out.write(enhanced_frame)\n",
        "\n",
        "        # Update progress every 1% of frames\n",
        "        current_frame += 1\n",
        "        progress = current_frame / total_frames\n",
        "        if current_frame % max(1, total_frames//100) == 0:\n",
        "            print(\"#\", end='', flush=True)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(\"]\\nProcessing completed!\")\n",
        "    print(f\"Enhanced video saved to: {output_path}\")\n",
        "\n",
        "# Example usage\n",
        "input_video = \"/kaggle/input/uwenhancementv3/underwater-image-enhancement-main/GX016431.MP4\"\n",
        "output_video = \"/kaggle/working/output1.mp4\"\n",
        "enhance_video(input_video, output_video, flag=1)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T18:23:52.506433Z",
          "iopub.execute_input": "2025-03-07T18:23:52.506718Z",
          "iopub.status.idle": "2025-03-07T18:39:20.130819Z",
          "shell.execute_reply.started": "2025-03-07T18:23:52.506695Z",
          "shell.execute_reply": "2025-03-07T18:39:20.129877Z"
        },
        "id": "lTPB8EnSLIBG"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}